{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries \n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    \"\"\"\n",
    "    Cleaning the text: removing punctuation, stopwords and \n",
    "    making them all lower letter.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = re.sub(r\"[^A-Za-z0-9^]\", \" \", doc)\n",
    "    doc = [word for word in doc.lower().split() if not word in stopwords.words(\"english\")]\n",
    "    doc = \" \".join(doc)\n",
    "    return doc\n",
    "\n",
    "def labels(data):\n",
    "    tags = []\n",
    "    for index, row in enumerate(data):\n",
    "        tags.append(LabeledSentence(row.split(), ['doc_%d'% index]))\n",
    "    return tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(path, size=300):\n",
    "    \"\"\"\n",
    "    Function for pre-processing using Doc2Vec. \n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.read_csv(path)\n",
    "    data = data.dropna(subset=['text']).reset_index().drop(['index', 'id'], axis=1)\n",
    "    \n",
    "    data['text'] = data['text'].apply(lambda row: clean(row))\n",
    "\n",
    "    X = labels(data['text'])\n",
    "    y = data['label'].values\n",
    "\n",
    "    model_t = Doc2Vec(min_count=1, vector_size = size, window=5, sample=1e-4, negative=5, \n",
    "                      workers=10, epochs=10, seed=0)\n",
    "    model_t.build_vocab(X)\n",
    "    model_t.train(X, total_examples=model_t.corpus_count, epochs=model_t.iter)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(model_t.docvecs, y, test_size=0.2, \n",
    "                                                        random_state=0, stratify=y)\n",
    "    X_train, X_test, y_train, y_test = np.array(X_train), np.array(X_test), \n",
    "                                       np.array(y_train), np.array(y_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Embedding, Input, RepeatVector\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scikitplot.plotters as skplt\n",
    "import os\n",
    "\n",
    "X_train, X_test, y_train, y_test = pre_process('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    '''Initializing the neural network.'''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=300, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(80, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(1, activation=\"sigmoid\", kernel_initializer='normal'))\n",
    "\n",
    "    #compiling a gradient descent algorithm\n",
    "    model.compile(loss='binary_crossentropy', optimizer= Adam(lr=0.01), metrics='accuracy')\n",
    "    return model\n",
    "\n",
    "#setting the model\n",
    "model = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = model.fit(X_train, y_train, epochs=20, batch_size=64, verbose=0)\n",
    "print(\"Model Trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = model.evaluate(X_train, y_train)\n",
    "test = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"Train Set Accuracy: {}\\nTest Set Accuracy: {} \".format(str(round(train[1]*100, 2)),\n",
    "                                                              str(round(test[1]*100, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [0 if x<0.5 else 1 for x in model.predict(X_test)]\n",
    "skplt.plot_confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
